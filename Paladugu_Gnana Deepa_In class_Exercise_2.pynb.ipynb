{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhnD5O7G56lr"
      },
      "source": [
        "# write your answer here\n",
        "Reasearch Question: How do movie ratings impact popularity of movies over time\n",
        "Data Required: Data required for the above abstract is movie name, date, rating and amount of data required\n",
        "is we need atleast 1000 records to perform statistical analysis and ideally we should have atleast 2000\n",
        "records for better analysis.\n",
        "Steps for collecting and saving the data:\n",
        "1. Setup- We have install all the necessary dependencies and we have to identify reliable website to perform our analysis\n",
        "2. Webscraping- We have write script such that our code will scrape data from multiple pages of the website from the dynamic link\n",
        "3. Saving data- After getting the itegrated data from multiple pages of website, we can convert the data\n",
        "into data frame and save it in the excel format and then visualize the data insights.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "4XvRknixTh1g",
        "outputId": "a5c8c54a-ba97-4d4b-887d-604a0ba6cba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed page 1. Total movies collected: 20\n",
            "Completed page 2. Total movies collected: 40\n",
            "Completed page 3. Total movies collected: 60\n",
            "Completed page 4. Total movies collected: 80\n",
            "Completed page 5. Total movies collected: 100\n",
            "Completed page 6. Total movies collected: 120\n",
            "Completed page 7. Total movies collected: 140\n",
            "Completed page 8. Total movies collected: 160\n",
            "Completed page 9. Total movies collected: 180\n",
            "Completed page 10. Total movies collected: 200\n",
            "Completed page 11. Total movies collected: 220\n",
            "Completed page 12. Total movies collected: 240\n",
            "Completed page 13. Total movies collected: 260\n",
            "Completed page 14. Total movies collected: 280\n",
            "Completed page 15. Total movies collected: 300\n",
            "Completed page 16. Total movies collected: 320\n",
            "Completed page 17. Total movies collected: 340\n",
            "Completed page 18. Total movies collected: 360\n",
            "Completed page 19. Total movies collected: 380\n",
            "Completed page 20. Total movies collected: 400\n",
            "Completed page 21. Total movies collected: 420\n",
            "Completed page 22. Total movies collected: 440\n",
            "Completed page 23. Total movies collected: 460\n",
            "Completed page 24. Total movies collected: 480\n",
            "Completed page 25. Total movies collected: 500\n",
            "Completed page 26. Total movies collected: 520\n",
            "Completed page 27. Total movies collected: 540\n",
            "Completed page 28. Total movies collected: 560\n",
            "Completed page 29. Total movies collected: 580\n",
            "Completed page 30. Total movies collected: 600\n",
            "Completed page 31. Total movies collected: 620\n",
            "Completed page 32. Total movies collected: 640\n",
            "Completed page 33. Total movies collected: 660\n",
            "Completed page 34. Total movies collected: 680\n",
            "Completed page 35. Total movies collected: 700\n",
            "Completed page 36. Total movies collected: 720\n",
            "Completed page 37. Total movies collected: 740\n",
            "Completed page 38. Total movies collected: 760\n",
            "Completed page 39. Total movies collected: 780\n",
            "Completed page 40. Total movies collected: 800\n",
            "Completed page 41. Total movies collected: 820\n",
            "Completed page 42. Total movies collected: 840\n",
            "Completed page 43. Total movies collected: 860\n",
            "Completed page 44. Total movies collected: 880\n",
            "Completed page 45. Total movies collected: 900\n",
            "Completed page 46. Total movies collected: 920\n",
            "Completed page 47. Total movies collected: 940\n",
            "Completed page 48. Total movies collected: 960\n",
            "Completed page 49. Total movies collected: 980\n",
            "Completed page 50. Total movies collected: 1000\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_43f9fad4-86e3-4ffd-97ff-57eb5a27de8a\", \"tmdb_movies_data.xlsx\", 34377)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping completed and Excel file downloaded.\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "base_url = 'https://www.themoviedb.org/movie?page='\n",
        "all_page_data = []\n",
        "\n",
        "for num in range(1, 51):  # Scraping the data for the first 50 pages\n",
        "    if len(all_page_data) >= 1000:\n",
        "        break\n",
        "\n",
        "    resp = requests.get(base_url + str(num))\n",
        "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "    all_div = soup.find_all('div', class_='card style_1')\n",
        "\n",
        "    for item in all_div:\n",
        "        if len(all_page_data) >= 1000:\n",
        "            break  # Breaking the loop after collecting the 1000 records\n",
        "\n",
        "        content_div = item.find('div', class_='content')\n",
        "\n",
        "        movie_name = content_div.find('h2').text.strip()\n",
        "        movie_date = content_div.find('p').text.strip()\n",
        "\n",
        "        rating_div = item.find('div', class_='user_score_chart')\n",
        "        rating = rating_div[\"data-percent\"] if rating_div else 'N/A'\n",
        "        movie_data = {\n",
        "            'Movie_name': movie_name,\n",
        "            'Release_date': movie_date,\n",
        "            'Rating': rating\n",
        "        }\n",
        "        all_page_data.append(movie_data)\n",
        "    print(f\"Completed page {num}. Total movies collected: {len(all_page_data)}\")\n",
        "df = pd.DataFrame(all_page_data)\n",
        "excel_filename = 'tmdb_movies_data.xlsx' # Saving data into excel sheet\n",
        "df.to_excel(excel_filename, index=False)\n",
        "files.download(excel_filename) # Downloading the Excel sheet\n",
        "print(\"Scraping completed and Excel file downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGLbSHHB8Ej",
        "outputId": "d1835bc6-bfc8-4813-95d5-b7bad70e0a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Request error on page 11: HTTPSConnectionPool(host='dl.acm.org', port=443): Read timed out.\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import os\n",
        "# Set up logging for output\n",
        "logging.basicConfig(filename='scraping_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# function for scraping 1000 articles from AMC digital library\n",
        "def scrape_acm_articles(search_query, limit=1000):\n",
        "    base_url = \"https://dl.acm.org/action/doSearch\" #base URL\n",
        "    params = {\n",
        "        \"AllField\": search_query,\n",
        "        \"startPage\": 1,\n",
        "        \"pageSize\": 50\n",
        "    }\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\",\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n",
        "    ]\n",
        "    collected_articles = []\n",
        "    current_page = 1\n",
        "    total_collected = 0\n",
        "    logging_interval = 100\n",
        "    # Retrying for blocked requests\n",
        "    retry_strategy = Retry(\n",
        "        total=5,\n",
        "        backoff_factor=2,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session = requests.Session()\n",
        "    session.mount(\"https://\", adapter)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    # checking the collected records be less than limit for every scrape\n",
        "    while total_collected < limit:\n",
        "        params[\"startPage\"] = current_page\n",
        "        headers = {\n",
        "            \"User-Agent\": random.choice(user_agents)\n",
        "        }\n",
        "        try:\n",
        "            response = session.get(base_url, params=params, headers=headers, timeout=20)\n",
        "            response.raise_for_status()\n",
        "        except requests.HTTPError as http_err:\n",
        "            logging.error(f\"HTTP error on page {current_page}: {http_err}\")\n",
        "            time.sleep(120)\n",
        "            current_page += 1\n",
        "            continue\n",
        "        except requests.RequestException as req_err:\n",
        "            logging.error(f\"Request error on page {current_page}: {req_err}\")\n",
        "            time.sleep(120)\n",
        "            current_page += 1\n",
        "            continue\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all(\"div\", class_=\"issue-item\")\n",
        "        if not articles:\n",
        "            logging.info(f\"No more articles found on page {current_page}.\")\n",
        "            break\n",
        "        page_articles_count = 0\n",
        "        for article in articles:\n",
        "            title_elem = article.find(\"h5\", class_=\"issue-item__title\")\n",
        "            title = title_elem.get_text(strip=True) if title_elem else \"Title unavailable\"\n",
        "            conference_elem = article.find(\"div\", class_=\"issue-item__detail\")\n",
        "            conference = \"Unknown\"\n",
        "            if conference_elem:\n",
        "                conf_link = conference_elem.find(\"a\")\n",
        "                if conf_link:\n",
        "                    conference = conf_link.get_text(strip=True)\n",
        "            year_elem = article.find(\"div\", class_=\"bookPubDate\")\n",
        "            year = year_elem.get_text(strip=True) if year_elem else \"Year unknown\"\n",
        "            authors_elem = article.find(\"ul\", class_=\"rlist--inline\")\n",
        "            authors = authors_elem.get_text(strip=True) if authors_elem else \"Authors unavailable\"\n",
        "            abstract_elem = article.find(\"div\", class_=\"issue-item__abstract\")\n",
        "            abstract = abstract_elem.get_text(strip=True) if abstract_elem else \"No abstract provided\"\n",
        "            collected_articles.append({\n",
        "                \"Title\": title,\n",
        "                \"Conference\": conference,\n",
        "                \"Year\": year,\n",
        "                \"Authors\": authors,\n",
        "                \"Abstract\": abstract\n",
        "            })\n",
        "            page_articles_count += 1\n",
        "            total_collected += 1\n",
        "            if total_collected % logging_interval == 0:\n",
        "                logging.info(f\"Collected {total_collected} articles\")\n",
        "            if total_collected >= limit: # condition to break out of loop after reaching 1000 records\n",
        "                break\n",
        "        logging.info(f\"Collected {page_articles_count} articles on page {current_page}.\")\n",
        "        current_page += 1\n",
        "        time.sleep(random.uniform(10, 20))\n",
        "    return collected_articles\n",
        "def save_to_excel(data, file_name):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(file_name, index=False)\n",
        "    logging.info(f\"Saved {len(data)} records to {file_name}\")\n",
        "def run_scraper():\n",
        "    search_query = \"machine learning\" #searching articles regarding machine learning\n",
        "    article_limit = 1000\n",
        "    logging.info(f\"Starting scraping: {search_query}\") #logging info for scraping\n",
        "    scraped_data = scrape_acm_articles(search_query, article_limit)\n",
        "    excel_filename = 'acm_articles_ml.xlsx'\n",
        "    save_to_excel(scraped_data, excel_filename) #saving to excel\n",
        "    print(f\"Scraping complete articles saved to {excel_filename}.\")\n",
        "    print(\"For detailed logs, check 'scraping_log.txt'.\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(excel_filename) # using google colab's download function\n",
        "        print(f\"{excel_filename} is ready for download in Colab.\")\n",
        "    except ImportError:\n",
        "        print(f\"File saved locally: {os.path.abspath(excel_filename)}\")\n",
        "if __name__ == \"__main__\":\n",
        "    run_scraper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtKskTzbCLaU",
        "outputId": "64291603-dd3b-477d-bff4-e3fc7e4723b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ntscraper\n",
            "  Downloading ntscraper-0.3.17-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.12.3)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.9.4)\n",
            "Requirement already satisfied: tqdm>=4.66 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (2024.8.30)\n",
            "Downloading ntscraper-0.3.17-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: ntscraper\n",
            "Successfully installed ntscraper-0.3.17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing instances: 100%|██████████| 16/16 [00:13<00:00,  1.19it/s]\n",
            "INFO:root:No instance specified, using random instance https://nitter.privacydev.net\n",
            "INFO:root:Current stats for MrBeast: 17 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 33 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 48 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 64 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 81 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 94 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 105 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 121 tweets, 0 threads...\n",
            "WARNING:root:Empty page on https://nitter.privacydev.net\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_8f5c7bce-8b8d-4450-822c-fe08b779c7a8\", \"MrBeast_tweets_data.xlsx\", 10243)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping completed\n"
          ]
        }
      ],
      "source": [
        "!pip install ntscraper\n",
        "from ntscraper import Nitter # importing nitter\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
        "tweets = scraper.get_tweets(\"MrBeast\", mode=\"user\", number=1000)\n",
        "# Intializing list which can store data of tweets\n",
        "tweet_data = {\n",
        "    'URL': [],\n",
        "    'Tweet Text': [],\n",
        "    'Username': [],\n",
        "    'Likes Count': [],\n",
        "    'Quotes Count': [],\n",
        "    'Retweets Count': [],\n",
        "    'Comments Count': []\n",
        "}\n",
        "# Limit to 40 tweets\n",
        "for i, tweet in enumerate(tweets['tweets']):\n",
        "    if i >= 40:\n",
        "        break\n",
        "    tweet_data['URL'].append(tweet['link'])\n",
        "    tweet_data['Tweet Text'].append(tweet['text'])\n",
        "    tweet_data['Username'].append(tweet['user']['name'])\n",
        "    tweet_data['Likes Count'].append(tweet['stats']['likes'])\n",
        "    tweet_data['Quotes Count'].append(tweet['stats']['quotes'])\n",
        "    tweet_data['Retweets Count'].append(tweet['stats']['retweets'])\n",
        "    tweet_data['Comments Count'].append(tweet['stats']['comments'])\n",
        "tweet_df = pd.DataFrame(tweet_data)\n",
        "output_filename = \"MrBeast_tweets_data.xlsx\"\n",
        "tweet_df.to_excel(output_filename, index=False)\n",
        "files.download(output_filename)\n",
        "print(\"Scraping completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "I enjoyed solving these tasks, It was challenging a bit because I was getting CAPTCHA for google scholar web site and I tried to resolve it by using API but it didn't work, then I tried for AMC digital library and got the expected output.\n",
        "This web scraping is very useful to get the live data and get all the insights by analyzing the data. In my research it can be use full to find the exact reviews on a particular film across different web sites.\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}